import json
import time
import uuid
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from ..models.schemas import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk, Choice, Delta, Message, Usage
from ..services.agent_factory import create_route_agent, create_chat_agent, create_tool_agent
from ..services.streaming import process_and_stream_response, stream_agent_response
from ..tools.search_tools import enhanced_search

router = APIRouter(prefix="/api", tags=["chat"])

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
route_agent = create_route_agent()
chat_agent = create_chat_agent()
tool_agent = create_tool_agent()

@router.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    try:
        user_message = request.messages[-1].content if request.messages else ""
        completion_id = f"chatcmpl-{str(uuid.uuid4())}"
        created_time = int(time.time())
        
        print(f"ì‚¬ìš©ì: {user_message}")
        
        async def ai_stream_generator():
            try:
                print("AI ì—ì´ì „íŠ¸ ì‹œì‘")
                
                # LLM ê¸°ë°˜ ë¼ìš°íŒ…
                print("ë¼ìš°íŒ… ê²°ì • ì¤‘...")
                route_agent.history = route_agent.base_history.copy()
                route_agent.history.append({"role": "user", "content": user_message})
                
                route_response = route_agent.client.chat.completions.create(
                    model=route_agent.model,
                    messages=route_agent.history,
                    tools=route_agent.tools,
                    tool_choice="auto",
                    stream=False
                )
                
                route = "chat"  # ê¸°ë³¸ê°’
                
                # ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ í™•ì¸
                if route_response.choices[0].message.tool_calls:
                    tool_call = route_response.choices[0].message.tool_calls[0]
                    if tool_call.function.name == "routing":
                        try:
                            args = json.loads(tool_call.function.arguments)
                            route = args.get("agent", "chat")
                        except:
                            route = "chat"
                else:
                    # í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ ì¶”ì¶œ
                    content = route_response.choices[0].message.content.lower()
                    if "tool" in content:
                        route = "tool"
                
                print(f"ğŸ“ ë¼ìš°íŒ… ê²°ê³¼: {route}")
                
                if route == "chat":
                    # ì±„íŒ… ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ
                    # ìš”ì²­ì˜ messages ì‚¬ìš© (ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€)
                    if request.messages:
                        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€ë¥¼ ìœ„í•´ base_historyì˜ ì²« ë²ˆì§¸ ë©”ì‹œì§€ ì‚¬ìš©
                        chat_agent.history = [chat_agent.base_history[0]]
                        # ì‚¬ìš©ìê°€ ë³´ë‚¸ ì „ì²´ ëŒ€í™” ë‚´ì—­ ì¶”ê°€
                        for msg in request.messages:
                            chat_agent.history.append({"role": msg.role, "content": msg.content})
                    else:
                        # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ìƒˆ ëŒ€í™” ì‹œì‘
                        chat_agent.history = chat_agent.base_history.copy()
                        chat_agent.history.append({"role": "user", "content": user_message})
                    
                    print(f"ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(chat_agent.history)}")
                    
                    response = chat_agent.client.chat.completions.create(
                        model=chat_agent.model,
                        messages=chat_agent.history,
                        stream=True
                    )
                    
                    accumulated_response = ""
                    for chunk in response:
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if delta.content:
                                accumulated_response += delta.content
                                
                                chunk_data = {
                                    "id": completion_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [
                                        {
                                            "index": 0,
                                            "delta": {
                                                "content": delta.content
                                            },
                                            "finish_reason": None
                                        }
                                    ]
                                }
                                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                            
                            if chunk.choices[0].finish_reason == "stop":
                                break
                    
                    # íˆìŠ¤í† ë¦¬ì— ì‘ë‹µ ì¶”ê°€
                    if accumulated_response:
                        chat_agent.history.append({"role": "assistant", "content": accumulated_response})
                
                else:
                    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                    print("ğŸ”§ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
                    tool_agent.history = tool_agent.base_history.copy()
                    tool_agent.history.append({"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”: {user_message}"})
                    
                    search_query_response = tool_agent.client.chat.completions.create(
                        model=tool_agent.model,
                        messages=tool_agent.history,
                        stream=False
                    )
                    
                    # ê²€ìƒ‰ì–´ ì¶”ì¶œ
                    search_query = search_query_response.choices[0].message.content
                    search_query = search_query.strip().strip('"').strip("'")
                    if "ê²€ìƒ‰ì–´:" in search_query:
                        search_query = search_query.split("ê²€ìƒ‰ì–´:")[-1].strip()
                    
                    print(f"ğŸ” ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    search_result = enhanced_search(search_query)
                    print(f"ğŸ“Š ê²€ìƒ‰ ì™„ë£Œ: {len(search_result)} ê¸€ì")
                    
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
                    final_prompt = f"""ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ê²€ìƒ‰ ê²°ê³¼: {search_result}

ì‚¬ìš©ì ì§ˆë¬¸: {user_message}"""
                    
                    # íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ê°œì„ 
                    if request.messages:
                        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€
                        chat_agent.history = [chat_agent.base_history[0]]
                        # ì´ì „ ëŒ€í™” ë‚´ì—­ ìœ ì§€ (ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ëŒ€ì²´)
                        for i, msg in enumerate(request.messages):
                            if i < len(request.messages) - 1:
                                chat_agent.history.append({"role": msg.role, "content": msg.content})
                        # ê²€ìƒ‰ ê²°ê³¼ê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                        chat_agent.history.append({"role": "user", "content": final_prompt})
                    else:
                        chat_agent.history = chat_agent.base_history.copy()
                        chat_agent.history.append({"role": "user", "content": final_prompt})
                    
                    print(f"ê²€ìƒ‰ ëª¨ë“œ íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(chat_agent.history)}")
                    
                    response = chat_agent.client.chat.completions.create(
                        model=chat_agent.model,
                        messages=chat_agent.history,
                        stream=True
                    )
                    
                    accumulated_response = ""
                    for chunk in response:
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if delta.content:
                                accumulated_response += delta.content
                                
                                chunk_data = {
                                    "id": completion_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [
                                        {
                                            "index": 0,
                                            "delta": {
                                                "content": delta.content
                                            },
                                            "finish_reason": None
                                        }
                                    ]
                                }
                                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                            
                            if chunk.choices[0].finish_reason == "stop":
                                break
                    chat_agent.history.append({"role": "assistant", "content": accumulated_response})
                
                # ì¢…ë£Œ ì²­í¬
                final_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }
                    ]
                }
                yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
                
                print("âœ… AI ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ AI ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {str(e)}")
                error_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {
                                "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            },
                            "finish_reason": "stop"
                        }
                    ]
                }
                yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            ai_stream_generator(), 
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*"
            }
        )
            
    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/ask/custom")
async def ask_custom(request: ChatCompletionRequest):
    """jh-chat ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ - OpenAI í˜¸í™˜ í˜•ì‹"""
    try:
        user_message = request.messages[-1].content if request.messages else ""
        completion_id = f"chatcmpl-{str(uuid.uuid4())}"
        created_time = int(time.time())
        
        async def stream_generator():
            try:
                print("ğŸš€ ìŠ¤íŠ¸ë¦¼ ì‹œì‘")
                chunk_count = 0
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì„¤ì •
                if request.messages:
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€
                    chat_agent.history = [chat_agent.base_history[0]]
                    # ì‚¬ìš©ìê°€ ë³´ë‚¸ ëª¨ë“  ë©”ì‹œì§€ ì¶”ê°€
                    for msg in request.messages:
                        chat_agent.history.append({"role": msg.role, "content": msg.content})
                
                # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                async for chunk_data in process_and_stream_response(user_message, completion_id, created_time, request.model, chat_agent):
                    if chunk_data and chunk_data.strip():
                        chunk_count += 1
                        print(f"ğŸ“¤ ì²­í¬ {chunk_count}: {len(chunk_data)} bytes")
                        yield chunk_data
                
                print(f"âœ… ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ (ì´ {chunk_count}ê°œ ì²­í¬)")
                
                # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                final_chunk = ChatCompletionChunk(
                    id=completion_id,
                    object="chat.completion.chunk",
                    created=created_time,
                    model=request.model,
                    choices=[
                        Choice(
                            index=0,
                            delta=Delta(),
                            finish_reason="stop"
                        )
                    ]
                )
                yield f"data: {final_chunk.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
                print("ğŸ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡")
                
            except Exception as e:
                print(f"âŒ ìŠ¤íŠ¸ë¦¼ ì œë„ˆë ˆì´í„° ì˜¤ë¥˜: {str(e)}")
                error_chunk = ChatCompletionChunk(
                    id=completion_id,
                    object="chat.completion.chunk",
                    created=created_time,
                    model=request.model,
                    choices=[
                        Choice(
                            index=0,
                            delta=Delta(content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"),
                            finish_reason="stop"
                        )
                    ]
                )
                yield f"data: {error_chunk.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            stream_generator(), 
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*"
            }
        )
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/ask/custom/abort")
async def abort_custom_chat():
    """jh-chat ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ ì¤‘ë‹¨ ìš”ì²­ ì²˜ë¦¬"""
    return {"message": "Chat aborted successfully"}

@router.get("/v1/models")
async def list_models():
    return {
        "object": "list",
        "data": [
            {
                "id": "Searching",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "custom"
            }
        ]
    } 