import json
import time
from ..models.schemas import ChatCompletionChunk, Choice, Delta
from ..tools.search_tools import enhanced_search

async def stream_agent_response(agent, prompt):
    """ì—ì´ì „íŠ¸ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°"""
    # ê¸°ì¡´ íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    agent.history.append({"role": "user", "content": prompt})
    
    if len(agent.history) > 10:
        agent.history = [agent.history[0]] + agent.history[-8:]
    
    params = {
        "model": agent.model,
        "messages": agent.history,
        "stream": True,
    }
    
    if agent.tools:
        params["tools"] = agent.tools
        params["tool_choice"] = "auto"
    
    try:
        response = agent.client.chat.completions.create(**params)
        
        accumulated_response = ""
        function_name = ""
        function_args = ""
        json_buffer = ""
        has_content = False
        
        for chunk in response:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                
                if delta.content:
                    accumulated_response += delta.content
                    has_content = True
                    yield delta.content  # ì‹¤ì‹œê°„ìœ¼ë¡œ ì²­í¬ ì „ì†¡
                    
                elif delta.tool_calls:
                    tool_call = delta.tool_calls[0]
                    
                    if tool_call.function.name:
                        function_name += tool_call.function.name
                        yield f"\nğŸ”§ ë„êµ¬ ì‹¤í–‰: {tool_call.function.name}\n"
                        
                    if tool_call.function.arguments:
                        json_buffer += tool_call.function.arguments
                        
                        if json_buffer.count('{') == json_buffer.count('}') and json_buffer.count('{') > 0:
                            function_args = json_buffer
                            json_buffer = ""
                
                # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ í™•ì¸
                if chunk.choices[0].finish_reason == "stop":
                    break
        
        # ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        if accumulated_response:
            agent.history.append({"role": "assistant", "content": accumulated_response})
        elif function_name and function_args:
            # ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ ì²˜ë¦¬
            tool_result = agent.get_tool_response(function_name, function_args)
            yield f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.\n\n"
        
        # ìŠ¤íŠ¸ë¦¼ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
        if not has_content and not function_name:
            yield "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        yield f"ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {str(e)}"

async def process_and_stream_response(user_prompt: str, completion_id: str, created_time: int, model: str, chat_agent):
    """ì‹¤ì‹œê°„ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°"""
    try:
        print(f"\nğŸ¯ ì‚¬ìš©ì ìš”ì²­: {user_prompt}")
        
        # ì„ì‹œë¡œ chatìœ¼ë¡œ ê³ ì • (ë””ë²„ê¹…ìš©)
        route = "chat"
        print(f"\n[ë¼ìš°íŒ…: {route}] (ì„ì‹œ ê³ ì •)\n")
        
        if route == "chat":
            # ì±„íŒ… ì—ì´ì „íŠ¸ ì‘ë‹µì„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
            async for chunk_text in stream_agent_response(chat_agent, user_prompt):
                if chunk_text and chunk_text.strip():
                    chunk = ChatCompletionChunk(
                        id=completion_id,
                        object="chat.completion.chunk",
                        created=created_time,
                        model=model,
                        choices=[
                            Choice(
                                index=0,
                                delta=Delta(content=chunk_text),
                                finish_reason=None
                            )
                        ]
                    )
                    yield f"data: {chunk.model_dump_json()}\n\n"
            
    except Exception as e:
        print(f"ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {str(e)}")
        error_text = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        chunk = ChatCompletionChunk(
            id=completion_id,
            object="chat.completion.chunk",
            created=created_time,
            model=model,
            choices=[
                Choice(
                    index=0,
                    delta=Delta(content=error_text),
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {chunk.model_dump_json()}\n\n" 